{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las deep neural networks, cada hidden layer corresponde a un tensor y por lo tanto, cada hidden node o hidden unit, es una unidad de ese tensor. La cantidad de nodos de cada hidden layer se conoce como **width**, y la cantidad de hidden layers en una deep neural network, se conoce como **depth**. Ambos valores son hyperparameters que son definidos al crear el modelo. Cada layer tiene n x m wights, donde n es la cantidad de nodos en la capa anterior y m es la cantidad de nodos de la capa, y por lo tanto, cada peso Wij corresponde a la conexion del nodo i con el nodo j.\n",
    "\n",
    "Dos transformaciones lineales consecutivas equivalen a una unica tranformacion lineal, es por eso que en cada conexion, luego de aplicar la combinacion lineal entre las entradas y los pesos, se agrega una no linearidad para generar la salida o la siguiente hidden unit. Es decir, entre capas, al aplicar los pesos a las entradas, lo que se esta haciendo es una combinacion lineal de los mismos, por lo tanto, si tenemos varias capas y en todas se esta aplicando esa combinacion lineal, todas estas pueden resumirse en una unica transformacion lineal. Es por eso que luego se agrega la no linearidad, para que esto no ocurra y puedan existir las deep networks. Estas no linearidades, son las llamadas funciones de activacion.\n",
    "\n",
    "La **funcion de activacion** transforman las entradas en salidas relacionadas a dichas entradas, pero de otro tipo. Hay distintas funciones de activacion que pueden usarse, como:\n",
    "- Sigmoid (logistic function): retorna un valor entre 0 y 1\n",
    "- TanH (hyperbolic tangent): retorna un valor entre -1 y 1\n",
    "- ReLu (rectified linear unit): retorna valores de 0 en adelante\n",
    "- Softmax: retorna valores entre 0 y 1 cuya suma es igual a 1 (probabilidades)\n",
    "\n",
    "A diferencia de las demas, que para calcular la salida aplican la funcion a cada valor del vector de entrada (resultante de la transformacion lineal), teniendo en cuenta unicamente ese valor, la funcion Softmax, al aplicarla a cada valor del vector entrada, tiene en cuenta todos los valores. Al retornar un vector de probabilidades, es generalmente usada como la funcion de activacion en la ultima capa (capa de salida) de las deep neural networks.\n",
    "\n",
    "En la **backpropagation**, lo que se hace es propagar los errores hacia atras, para averiguar cuanto contribuye cada uno de los pesos en cada error, y en funcion de una mayor o menor contribucion, ajustarlos mas o menos\n",
    "\n",
    "\n",
    "Al entrenar un modelo, una buena forma de detectar **overfitting**, es separar el dataset en 3 partes, **training, validation** and **test**. El modelo se entrena con el training set, es decir, se ejecuta forward propagation y backward propagation para ajustar los pesos y luego se ejecuta unicamente forward propagation con el validation set para comparar el loss. Esto se realiza por cada epoch, y en cada uno de ellos, el loss del modelo con ambos sets de datos, deberia ir disminuyendo. Si en algun momento el loss del training set continua disminuyendo pero el loss del validation set comienza a aumentar, significa que se esta comenzando a producir overfitting, ya que esto indica que el modelo se esta ajustando demasiado a los datos del training set, por lo que su performance con el validation set disminuye. El test set se utiliza una vez completados todos los epochs para validar la precision del modelo con datos nunca antes vistos.\n",
    "\n",
    "En caso de no poseer un data set muy grande, una alternativa es aplicar **K-Fold Cross Validation**, que lo que hace es dividir el dataset solo en training y test, pero luego, al training set, lo divide en K subsets, utilizando K-1 para training, y el restante para validation, alternando este ultimo en cada epoch. La desventaja es que a diferencia del caso anterior, el validation set tambien se esta utilizando para entrenar el modelo, ya que el subset que en un epoch se utiliza para unicamente validation, en el siguiente epoch se utilizara como training set. Por lo tanto, es mas propicio a overfitting.\n",
    "\n",
    "Al **inicializar los pesos** de la red, si establecemos el mismo valor para todos, todos los valores resultantes de los nodos serian iguales y todos contribuirian en igual cantidad al error, por lo que en la backpropagation, serian actualizados de igual manera y por ende, no tendria sentido. Una forma de inicializarlos, es elegir valores aleatorios entre -1 y 1 a partir de alguna distribucion. Sin embargo, es conveniente que dichos valores no sean muy peque√±os (cercanos a cero) o muy grandes (cercanos a los extremos) ya que por ejemplo la funcion de activacion Sigmoid, se comporta de forma casi lineal en valores cercanos a cero, y tiende a 0 y 1 en los extremos, lo que no es muy conveniente. un metodo muy usado que evita esto, es la **Xavier initialization** (es el inicializador por defecto en tensorflow).\n",
    "\n",
    "Durante el proceso de **optimizacion**, al utilizar gradient descent, los pesos se actualizan una vez por epoch, al final del mismo, por lo que lograr la convergencia (obtener el loss minimo) puede resultar muy lento. Una alternativa, es utilizar el **Stochastic Gradient Descent**, que lo que hace es actualizar los pesos varias veces en un mismo epoch, cada cierta cantidad de muestras (batches), por lo que converge mas rapido. Para evitar caer en un minimo local, una solucion es tener en cuenta el **Momentum**, esto se refiere a considerar la \"velocidad\" con la que se venia descenciendo para calcular el siguiente valor durante el proceso de busqueda del minimo de la funcion. De esta forma, si se venia con mucha velocidad y estamos en un minimo local, es muy probable que podamos saltarlo para continuar en la busqueda del minimo global.\n",
    "\n",
    "Durante la etapa de data **preprocessing**, generalmente se aplica **standardization** (o tambien llamado feature scaling) para convertir variables de diferentes magnitudes, a una escala comun. Esta consiste en en sustraer a cada valor, la media correspondiente y luego dividir por la desviacion estandar. Como resultado se obtienen valores con una media de 0 y una desviacion estandar de 1. Esta es una de las tecnicas mas usadas, si bien existen otras tambien, pero por lo general depende del problema que se este tratando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
