{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripcion del Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema consiste en predecir si un usuario de una aplicacion de audiolibros, es probable que vuelva a comprar un libro o no, para de esto modo, apuntar la publicidad a aquellos que seguramente vuelvan a comprar.\n",
    "\n",
    "Para eso, el dataset consiste en una serie de datos de comportamiento de los usuarios, recopilados mediante la aplicacion durante 2 años. Luego, la variable dependiente o el target, es un valor entre 0 y 1 que indica si en los 6 meses siguientes, el usuario compro otro libro o no, ya que aquellos que luego de 2 años compraron otro libro, muy probablemente lo vuelvan a hacer, mientras que el resto, si durante esos 6 meses no compraron nada, es muy probable que no esten usando mas la aplicacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "unscaled_inputs = raw_csv_data[:, 1:-1]\n",
    "targets = raw_csv_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceando el Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancear el dataset consiste en igualar el numero de muestras para cada uno de los valores de salida, ya que si por ejemplo tenemos 2 posibles valores de salida pero hay mas muestras con un valor de salida que con el otro, el primero tendra mas peso por asi decirlo, por lo tanto el modelo se dara cuenta que la mayor cantidad de veces, las muestras corresponden a ese valor y comenzara a retornarlo mas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets))\n",
    "zero_targets_count = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets.shape[0]):\n",
    "    if zero_targets_count < num_one_targets:\n",
    "        if targets[i] == 0:\n",
    "            zero_targets_count += 1\n",
    "            indices_to_remove.append(i)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "unscaled_inputs_balanced = np.delete(unscaled_inputs, indices_to_remove, axis=0)\n",
    "targets_balanced = np.delete(targets, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satandardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mezclan los datos para que en caso de que estuvieran ordenados por algun parametro o por la salida, se rompa ese orden y el batching funcione bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea un array con indices de 0 a la cantidad de muestras del dataset y se mezcla\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# se generan los datasets mezclados usando como indices los creados anteriormente\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_balanced[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando el dataset en Training, Validation y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count + validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count + validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "\n",
    "validation_inputs = npz['inputs'].astype(np.float)\n",
    "validation_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "test_inputs = npz['inputs'].astype(np.float)\n",
    "test_targets = npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9477 samples, validate on 1184 samples\n",
      "Epoch 1/100\n",
      "9477/9477 - 1s - loss: 0.4492 - accuracy: 0.8087 - val_loss: 0.3302 - val_accuracy: 0.8809\n",
      "Epoch 2/100\n",
      "9477/9477 - 0s - loss: 0.2954 - accuracy: 0.8925 - val_loss: 0.2902 - val_accuracy: 0.8902\n",
      "Epoch 3/100\n",
      "9477/9477 - 0s - loss: 0.2697 - accuracy: 0.8987 - val_loss: 0.2681 - val_accuracy: 0.8919\n",
      "Epoch 4/100\n",
      "9477/9477 - 0s - loss: 0.2567 - accuracy: 0.9030 - val_loss: 0.2680 - val_accuracy: 0.8961\n",
      "Epoch 5/100\n",
      "9477/9477 - 0s - loss: 0.2528 - accuracy: 0.9027 - val_loss: 0.2590 - val_accuracy: 0.8944\n",
      "Epoch 6/100\n",
      "9477/9477 - 0s - loss: 0.2464 - accuracy: 0.9044 - val_loss: 0.2540 - val_accuracy: 0.9037\n",
      "Epoch 7/100\n",
      "9477/9477 - 0s - loss: 0.2422 - accuracy: 0.9060 - val_loss: 0.2500 - val_accuracy: 0.8978\n",
      "Epoch 8/100\n",
      "9477/9477 - 0s - loss: 0.2399 - accuracy: 0.9072 - val_loss: 0.2389 - val_accuracy: 0.9012\n",
      "Epoch 9/100\n",
      "9477/9477 - 0s - loss: 0.2370 - accuracy: 0.9074 - val_loss: 0.2401 - val_accuracy: 0.9029\n",
      "Epoch 10/100\n",
      "9477/9477 - 0s - loss: 0.2369 - accuracy: 0.9080 - val_loss: 0.2415 - val_accuracy: 0.9037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255ca32ba88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "\n",
    "# este es un callback que cortara el proceso de entrenamiento cuando se detecte que la validation loss aumenta respecto\n",
    "# del epoch anterior (o de n epochs anteriores, definidos con el parametro patience), para prevenir overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(\n",
    "    train_inputs, \n",
    "    train_targets, \n",
    "    batch_size=batch_size, \n",
    "    epochs=max_epochs, \n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(validation_inputs, validation_targets), \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186/1186 [==============================] - 0s 37us/sample - loss: 0.2429 - accuracy: 0.9030\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.24. - Test accuracy:  90.30%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0: .2f}. - Test accuracy: {1: .2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
